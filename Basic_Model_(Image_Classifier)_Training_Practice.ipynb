{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic Model (Image Classifier) Practice.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPfPCByR6mFvU2R3927OvjP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hazelhkim/Pytorch/blob/master/Basic_Model_(Image_Classifier)_Training_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0kJ8HIgI1ob",
        "colab_type": "text"
      },
      "source": [
        "# What We Need\n",
        "1. Model.py\n",
        "2. Dataloader.py\n",
        "3. Trainer.py\n",
        "4. Train.py\n",
        "5. Predict.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay38U3CnJIyy",
        "colab_type": "text"
      },
      "source": [
        "# 1. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5p1juBfeJW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ImageClassifier(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, 500),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(500),\n",
        "            nn.Linear(500, 400),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(400),\n",
        "            nn.Linear(400, 300),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(300),\n",
        "            nn.Linear(300, 200),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(200),\n",
        "            nn.Linear(200, 100),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.Linear(100, 50),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(50),\n",
        "            nn.Linear(50, output_size),\n",
        "            nn.Softmax(dim=-1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      y = self.layers(x)\n",
        "      return y"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSrbB9R8Tw5l",
        "colab_type": "text"
      },
      "source": [
        "# 2. DataLoader\n",
        "We'll do MNIST dataset here for practice. DataLoader will be used in Train.py part later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTPXelbqkAqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "from argparse import Namespace\n",
        "config = {\n",
        "    \"n_epochs\": 20,\n",
        "    'batch_size': 80,\n",
        "    'train_ratio': .8,\n",
        "    'verbose': 1,\n",
        "    'gpu_id': 0 if torch.cuda.is_available() else -1,\n",
        "}\n",
        "config = Namespace(**config)\n",
        "device = torch.device('cpu') if config.gpu_id <0 else torch.device('cuda:%d' % config.gpu_id)\n",
        "\n",
        "dataset = datasets.MNIST(\n",
        "    '../data',\n",
        "    train=True,\n",
        "    transform = transforms.Compose([transforms.ToTensor()]),\n",
        "    download = True\n",
        ")\n",
        "\n",
        "x = dataset.data.float()/255.\n",
        "x = x.view(x.size(0), -1).to(device) # Flatten all the data for x.\n",
        "y = dataset.targets.to(device)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCJ5vzoxBHSt",
        "colab_type": "text"
      },
      "source": [
        "# 3. Trainer.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJVIxU2Bm1fW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "\n",
        "def _train(model, optimizer, crit, x, y, config):\n",
        "  model.train()\n",
        "\n",
        "  # Shuffle before training\n",
        "  indices = torch.randperm(x.size(0), device = x.device)\n",
        "  x = torch.index_select(x, dim=0, index=indices).split(config.batch_size, dim=0)\n",
        "  y = torch.index_select(y, dim=0, index=indices).split(config.batch_size, dim=0)\n",
        "\n",
        "  total_loss = 0\n",
        "\n",
        "  for i, (x_i, y_i) in enumerate(zip(x, y)):\n",
        "    y_hat_i = model(x_i)\n",
        "    loss_i = crit(y_hat_i, y_i.squeeze()) # y_i might have unncessary extra space other than dim=0.\n",
        "\n",
        "    # Initialize the gradients of the model.\n",
        "    optimizer.zero_grad()\n",
        "    loss_i.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if config.verbose >= 2:\n",
        "      print(\"Train Iteration(%d/%d): loss=%.4e\" % (i+1, len(x, float(loss_i))))\n",
        "    \n",
        "    # Don't forget to detach to prevent memory leak.\n",
        "    total_loss += float(loss_i)\n",
        "\n",
        "  return total_loss/len(x)\n",
        "\n",
        "def _validate(model, optimizer, crit, x, y, config):\n",
        "  # Turn evaluation(validation) mode on.\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    # Shuffle before validating. (Shuffling is not necessary for validation but splittin is.)\n",
        "    indices = torch.randperm(x.size(0), device = x.device)\n",
        "    x = torch.index_select(x, dim=0, index=indices).split(config.batch_size, dim=0)\n",
        "    y = torch.index_select(y, dim=0, index=indices).split(config.batch_size, dim=0)\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (x_i, y_i) in enumerate(zip(x, y)): # for each batch\n",
        "    # Turn on the no_grad mode to make more efficiently \n",
        "      y_hat_i = model(x_i)\n",
        "      loss_i = crit(y_hat_i, y_i.squeeze())\n",
        "\n",
        "    if config.verbose >= 2:\n",
        "      print(\"Validate Iteration (%d/%d): loss=%.4e\" %(i+1, len(x), float(loss_i)))\n",
        "\n",
        "    # Don't forget to detach to prevent memory leak.\n",
        "    total_loss += float(loss_i)\n",
        "\n",
        "  return  total_loss / len(x)\n",
        "\n",
        "def train(model, optimizer, crit, x, y, config):\n",
        "\n",
        "  lowest_loss = np.inf\n",
        "  best_model = None\n",
        "\n",
        "  # Shuffle \n",
        "  indices = torch.randperm(x.size(0), device = x.device)\n",
        "  x = torch.index_select(x, dim=0, index = indices).split([int(x.size(0)*config.train_ratio), int(x.size(0) - x.size(0)*config.train_ratio)], dim=0)\n",
        "  y = torch.index_select(y, dim=0, index = indices).split([int(y.size(0)*config.train_ratio), int(y.size(0) - y.size(0)*config.train_ratio)], dim=0)\n",
        "\n",
        "\n",
        "  # Split the data for training and validating\n",
        "  ## The method below is not working since the train_ratio is not an exact integer but float even though it was initialized as a solid percentage such as 0.8(80%)\n",
        "  ## x = torch.split(x, (x.size(0)*config.train_ratio, x.size(0)*(1-config.train_ratio)), dim=0)\n",
        "  ## y = torch.split(y, (y.size(0)*config.train_ratio, y.size(0)*(1-config.train_ratio)), dim=0)\n",
        "  ###x = torch.split(x, [int(x.size(0)*config.train_ratio), int(x.size(0) - x.size(0)*config.train_ratio)], dim=0)\n",
        "  ###y = torch.split(y, [int(y.size(0)*config.train_ratio), int(y.size(0) - y.size(0)*config.train_ratio)], dim=0)\n",
        "\n",
        "  for i in range(config.n_epochs):\n",
        "\n",
        "    train_loss = _train(model, optimizer, crit, x[0], y[0], config)\n",
        "    valid_loss = _validate(model, optimizer, crit, x[1], y[1], config)\n",
        "\n",
        "    # You must use deepcopy to take a snapshot of current best weights.\n",
        "    if valid_loss <= lowest_loss:\n",
        "      lowest_loss = valid_loss\n",
        "      best_model = deepcopy(model.state_dict())\n",
        "\n",
        "    print(\"Epoch(%d/%d): train_loss=%.4e  valid_loss=%.4e lowest_loss=%.4e\" %(\n",
        "        i+1, \n",
        "        config.n_epochs,\n",
        "        train_loss,\n",
        "        valid_loss,\n",
        "        lowest_loss,\n",
        "        ))\n",
        "    \n",
        "    # Restore to best model.\n",
        "    model.load_state_dict(best_model)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4PD2UpoNl8x",
        "colab_type": "text"
      },
      "source": [
        "# 4. Train.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9yiHuSHth-I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "bb139948-ae0e-4a96-b9bf-d51c6e0cd22e"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = ImageClassifier(28*28, 10).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "train(model, optimizer, crit, x, y, config)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch(1/20): train_loss=1.5744e+00  valid_loss=1.0143e-02 lowest_loss=1.0143e-02\n",
            "Epoch(2/20): train_loss=1.5248e+00  valid_loss=1.0240e-02 lowest_loss=1.0143e-02\n",
            "Epoch(3/20): train_loss=1.5267e+00  valid_loss=9.9128e-03 lowest_loss=9.9128e-03\n",
            "Epoch(4/20): train_loss=1.5177e+00  valid_loss=9.9917e-03 lowest_loss=9.9128e-03\n",
            "Epoch(5/20): train_loss=1.5201e+00  valid_loss=1.0390e-02 lowest_loss=9.9128e-03\n",
            "Epoch(6/20): train_loss=1.5175e+00  valid_loss=1.0457e-02 lowest_loss=9.9128e-03\n",
            "Epoch(7/20): train_loss=1.5212e+00  valid_loss=1.0165e-02 lowest_loss=9.9128e-03\n",
            "Epoch(8/20): train_loss=1.5212e+00  valid_loss=9.9934e-03 lowest_loss=9.9128e-03\n",
            "Epoch(9/20): train_loss=1.5219e+00  valid_loss=9.9306e-03 lowest_loss=9.9128e-03\n",
            "Epoch(10/20): train_loss=1.5236e+00  valid_loss=9.9086e-03 lowest_loss=9.9086e-03\n",
            "Epoch(11/20): train_loss=1.5125e+00  valid_loss=1.0285e-02 lowest_loss=9.9086e-03\n",
            "Epoch(12/20): train_loss=1.5143e+00  valid_loss=9.9294e-03 lowest_loss=9.9086e-03\n",
            "Epoch(13/20): train_loss=1.5148e+00  valid_loss=9.8952e-03 lowest_loss=9.8952e-03\n",
            "Epoch(14/20): train_loss=1.5092e+00  valid_loss=9.8723e-03 lowest_loss=9.8723e-03\n",
            "Epoch(15/20): train_loss=1.5022e+00  valid_loss=9.9644e-03 lowest_loss=9.8723e-03\n",
            "Epoch(16/20): train_loss=1.5048e+00  valid_loss=1.0147e-02 lowest_loss=9.8723e-03\n",
            "Epoch(17/20): train_loss=1.5069e+00  valid_loss=1.0079e-02 lowest_loss=9.8723e-03\n",
            "Epoch(18/20): train_loss=1.5065e+00  valid_loss=1.0327e-02 lowest_loss=9.8723e-03\n",
            "Epoch(19/20): train_loss=1.5063e+00  valid_loss=9.8626e-03 lowest_loss=9.8626e-03\n",
            "Epoch(20/20): train_loss=1.5027e+00  valid_loss=9.8806e-03 lowest_loss=9.8626e-03\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}